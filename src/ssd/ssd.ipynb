{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab851108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import time\n",
    "from cv2 import cv2\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from cv2 import cv2\n",
    "from xml.etree import ElementTree as ET\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.autograd import Function\n",
    "import types\n",
    "import pandas as pd\n",
    "from numpy import random\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2d54424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyDrive\n",
      "  Downloading PyDrive-1.3.1.tar.gz (987 kB)\n",
      "\u001b[K     |################################| 987 kB 2.0 MB/s eta 0:00:01     |#######                         | 235 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-api-python-client>=1.2\n",
      "  Downloading google_api_python_client-2.19.1-py2.py3-none-any.whl (7.4 MB)\n",
      "\u001b[K     |################################| 7.4 MB 8.6 MB/s eta 0:00:01     |##                              | 665 kB 8.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting oauth2client>=4.0.0\n",
      "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "\u001b[K     |################################| 98 kB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyYAML>=3.0\n",
      "  Downloading PyYAML-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (640 kB)\n",
      "\u001b[K     |################################| 640 kB 7.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-api-core<3.0.0dev,>=1.21.0\n",
      "  Downloading google_api_core-2.0.1-py2.py3-none-any.whl (92 kB)\n",
      "\u001b[K     |################################| 92 kB 394 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<3.0.0dev,>=1.16.0\n",
      "  Downloading google_auth-2.0.2-py2.py3-none-any.whl (152 kB)\n",
      "\u001b[K     |################################| 152 kB 50.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting uritemplate<4dev,>=3.0.0\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Collecting google-auth-httplib2>=0.1.0\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting httplib2<1dev,>=0.15.0\n",
      "  Downloading httplib2-0.19.1-py3-none-any.whl (95 kB)\n",
      "\u001b[K     |################################| 95 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.12.0\n",
      "  Downloading protobuf-3.17.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |################################| 1.0 MB 10.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests<3.0.0dev,>=2.18.0\n",
      "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "\u001b[K     |################################| 62 kB 1.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting googleapis-common-protos<2.0dev,>=1.6.0\n",
      "  Downloading googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |################################| 198 kB 14.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/site-packages (from google-api-core<3.0.0dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (58.0.0)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |################################| 155 kB 22.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from google-auth-httplib2>=0.1.0->google-api-python-client>=1.2->PyDrive) (1.16.0)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in /usr/local/lib/python3.6/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client>=1.2->PyDrive) (2.4.7)\n",
      "Collecting pyasn1>=0.1.7\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |################################| 77 kB 4.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.4-py3-none-any.whl (36 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
      "\u001b[K     |################################| 145 kB 15.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.2-py3-none-any.whl (59 kB)\n",
      "\u001b[K     |################################| 59 kB 3.9 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
      "\u001b[K     |################################| 138 kB 23.0 MB/s eta 0:00:01\n",
      "\u001b[?25hUsing legacy 'setup.py install' for PyDrive, since package 'wheel' is not installed.\n",
      "Installing collected packages: pyasn1, urllib3, rsa, pyasn1-modules, protobuf, idna, charset-normalizer, certifi, cachetools, requests, httplib2, googleapis-common-protos, google-auth, uritemplate, google-auth-httplib2, google-api-core, PyYAML, oauth2client, google-api-python-client, PyDrive\n",
      "    Running setup.py install for PyDrive ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed PyDrive-1.3.1 PyYAML-5.4.1 cachetools-4.2.2 certifi-2021.5.30 charset-normalizer-2.0.4 google-api-core-2.0.1 google-api-python-client-2.19.1 google-auth-2.0.2 google-auth-httplib2-0.1.0 googleapis-common-protos-1.53.0 httplib2-0.19.1 idna-3.2 oauth2client-4.1.3 protobuf-3.17.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.26.0 rsa-4.7.2 uritemplate-3.0.1 urllib3-1.26.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install PyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1e472c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datapath_list(rootpath):\n",
    "    imgpath_template = osp.join(rootpath, 'JPEGImages', '%s.jpg')\n",
    "    annopath_template = osp.join(rootpath, 'Annotations', '%s.xml')\n",
    "    \n",
    "    train_id_names = osp.join(rootpath + 'ImageSets/Main/train.txt')\n",
    "    val_id_names = osp.join(rootpath + 'ImageSets/Main/val.txt')\n",
    "    \n",
    "    train_img_list = list()\n",
    "    train_anno_list = list()\n",
    "    \n",
    "    for line in open(train_id_names):\n",
    "        file_id = line.strip()\n",
    "        img_path = (imgpath_template % file_id)\n",
    "        anno_path = (annopath_template % file_id)\n",
    "        train_img_list.append(img_path)\n",
    "        train_anno_list.append(anno_path)\n",
    "        \n",
    "    val_img_list = list()\n",
    "    val_anno_list = list()\n",
    "    \n",
    "    for line in open(val_id_names):\n",
    "        file_id = line.strip()\n",
    "        img_path = (imgpath_template % file_id)\n",
    "        anno_path = (annopath_template % file_id)\n",
    "        val_img_list.append(img_path)\n",
    "        val_anno_list.append(anno_path)\n",
    "        \n",
    "    return train_img_list, train_anno_list, val_img_list, val_anno_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea3a9282",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Anno_xml2list(object):\n",
    "    def __init__(self, classes):\n",
    "        self.classes = classes\n",
    "    \n",
    "    def __call__(self, xml_path, width, height):\n",
    "        ret = []\n",
    "        xml = ET.parse(xml_path).getroot()\n",
    "        \n",
    "        for obj in xml.iter('object'):\n",
    "            difficult = int(obj.find('difficult').text)\n",
    "            if difficult == 1:\n",
    "                continue\n",
    "                \n",
    "            bndbox = []\n",
    "            \n",
    "            name = obj.find('name').text.lower().split()\n",
    "            bbox = obj.find('bndbox')\n",
    "            \n",
    "            pts = ['xmin', 'ymin', 'xmax', 'ymax']\n",
    "            \n",
    "            for pt in (pts):\n",
    "                cur_pixel = int(bbox.find(pt).text) - 1\n",
    "                \n",
    "                if pt == 'xmin' or pt == 'xmax':\n",
    "                    cur_pixel /= width\n",
    "                else:\n",
    "                    cur_pixel /= height\n",
    "                    \n",
    "                bndbox.append(cur_pixel)\n",
    "                \n",
    "            label_idx = self.classes.index(name[0])\n",
    "            bndbox.append(label_idx)\n",
    "            \n",
    "            ret += [bndbox]\n",
    "            \n",
    "        return np.array(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c49e712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_augumentation import Compose, ConvertFromInts, ToAbsoluteCoords, PhotometricDistort, Expand, RandomSampleCrop, RandomMirror, ToPercentCoords, Resize, SubtractMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "178e1aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransform:\n",
    "    def __init__(self, input_size, color_mean):\n",
    "        self.data_transform = {\n",
    "            'train': Compose([\n",
    "                              ConvertFromInts(),\n",
    "                              ToAbsoluteCoords(),\n",
    "                              PhotometricDistort(),\n",
    "                              Expand(color_mean),\n",
    "                              RandomSampleCrop(),\n",
    "                              RandomMirror(),\n",
    "                              ToPercentCoords(),\n",
    "                              Resize(input_size),\n",
    "                              SubtractMeans(color_mean)\n",
    "            ]),\n",
    "            'val': Compose([\n",
    "                            ConvertFromInts(),\n",
    "                            Resize(input_size),\n",
    "                            SubtractMeans(color_mean)\n",
    "            ])\n",
    "        }\n",
    "\n",
    "    def __call__(self, img, phase, boxes, labels):\n",
    "        return self.data_transform[phase](img, boxes, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71f4d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataset(data.Dataset):\n",
    "    def __init__(self, img_list, anno_list, phase, transform, transform_anno):\n",
    "        self.img_list = img_list\n",
    "        self.anno_list = anno_list\n",
    "        self.phase = phase\n",
    "        self.transform = transform\n",
    "        self.transform_anno = transform_anno\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        im, gt, h, w = self.pull_item(index)\n",
    "        return im, gt\n",
    "\n",
    "    def pull_item(self, index):\n",
    "        image_file_path = self.img_list[index]\n",
    "        img = cv2.imread(image_file_path)\n",
    "        height, width, channels = img.shape\n",
    "\n",
    "        anno_file_path = self.anno_list[index]\n",
    "        anno_list = self.transform_anno(anno_file_path, width, height)\n",
    "\n",
    "        img, boxes, labels = self.transform(\n",
    "            img, self.phase, anno_list[:, :4], anno_list[:, 4]\n",
    "        )\n",
    "        img = torch.from_numpy(img[:, :, (2, 1, 0)]).permute(2, 0, 1)\n",
    "\n",
    "        gt = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n",
    "\n",
    "        return img, gt, height, width\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4499c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def od_collate_fn(batch):\n",
    "  targets = []\n",
    "  imgs = []\n",
    "  for sample in batch:\n",
    "    imgs.append(sample[0])\n",
    "    targets.append(torch.FloatTensor(sample[1]))\n",
    "\n",
    "  imgs = torch.stack(imgs, dim=0)\n",
    "  return imgs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2229def5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vgg():\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "\n",
    "    cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'MC', 512, 512, 512, 'M', 512, 512, 512]\n",
    "\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "          layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        elif v == 'MC':\n",
    "          layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n",
    "        else:\n",
    "          conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "          layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "          in_channels = v\n",
    "\n",
    "    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
    "    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
    "    layers += [pool5, conv6, nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n",
    "\n",
    "    return nn.ModuleList(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97e20c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_extras():\n",
    "    layers = []\n",
    "    in_channels = 1024\n",
    "\n",
    "    cfg = [256, 512, 128, 256, 128, 256, 128, 256]\n",
    "\n",
    "    layers += [nn.Conv2d(in_channels, cfg[0], kernel_size=(1))]\n",
    "    layers += [nn.Conv2d(cfg[0], cfg[1], kernel_size=(3), stride=2, padding=1)]\n",
    "    layers += [nn.Conv2d(cfg[1], cfg[2], kernel_size=(1))]\n",
    "    layers += [nn.Conv2d(cfg[2], cfg[3], kernel_size=(3), stride=2, padding=1)]\n",
    "    layers += [nn.Conv2d(cfg[3], cfg[4], kernel_size=(1))]\n",
    "    layers += [nn.Conv2d(cfg[4], cfg[5], kernel_size=(3))]\n",
    "    layers += [nn.Conv2d(cfg[5], cfg[6], kernel_size=(1))]\n",
    "    layers += [nn.Conv2d(cfg[6], cfg[7], kernel_size=(3))]\n",
    "\n",
    "    return nn.ModuleList(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "712b6f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loc_conf(num_classes=21, bbox_aspect_num=[4, 6, 6, 6, 4, 4]):\n",
    "    loc_layers = []\n",
    "    conf_layers = []\n",
    "\n",
    "    loc_layers += [nn.Conv2d(512, bbox_aspect_num[0] * 4, kernel_size=3, padding=1)]\n",
    "    conf_layers += [nn.Conv2d(512, bbox_aspect_num[0] * num_classes, kernel_size=3, padding=1)]\n",
    "\n",
    "    loc_layers += [nn.Conv2d(1024, bbox_aspect_num[1] * 4, kernel_size=3, padding=1)]\n",
    "    conf_layers += [nn.Conv2d(1024, bbox_aspect_num[1] * num_classes, kernel_size=3, padding=1)]\n",
    "\n",
    "    loc_layers += [nn.Conv2d(512, bbox_aspect_num[2] * 4, kernel_size=3, padding=1)]\n",
    "    conf_layers += [nn.Conv2d(512, bbox_aspect_num[2] * num_classes, kernel_size=3, padding=1)]\n",
    "\n",
    "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[3] * 4, kernel_size=3, padding=1)]\n",
    "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[3] * num_classes, kernel_size=3, padding=1)]\n",
    "\n",
    "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[4] * 4, kernel_size=3, padding=1)]\n",
    "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[4] * num_classes, kernel_size=3, padding=1)]\n",
    "\n",
    "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[5] * 4, kernel_size=3, padding=1)]\n",
    "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[5] * num_classes, kernel_size=3, padding=1)]\n",
    "\n",
    "    return nn.ModuleList(loc_layers), nn.ModuleList(conf_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e94f54bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2Norm(nn.Module):\n",
    "    def __init__(self, input_channels=512, scale=20):\n",
    "        super(L2Norm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(input_channels))\n",
    "        self.scale = scale\n",
    "        self.reset_parameters()\n",
    "        self.eps = 1e-10\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.constant_(self.weight, self.scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()+self.eps\n",
    "        x = torch.div(x, norm)\n",
    "\n",
    "        weights = self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
    "        out = weights * x\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeab3b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBox(object):\n",
    "    def __init__(self, cfg):\n",
    "        super(DBox, self).__init__()\n",
    "\n",
    "        self.image_size = cfg['input_size']\n",
    "        self.feature_maps = cfg['feature_maps']\n",
    "        self.num_priors = len(cfg['feature_maps'])\n",
    "        self.steps = cfg['steps']\n",
    "        self.min_sizes = cfg['min_sizes']\n",
    "        self.max_sizes = cfg['max_sizes']\n",
    "        self.aspect_ratios = cfg['aspect_ratios']\n",
    "\n",
    "    def make_dbox_list(self):\n",
    "        mean = []\n",
    "        for k, f in enumerate(self.feature_maps):\n",
    "            for i, j in product(range(f), repeat=2):\n",
    "                f_k = self.image_size / self.steps[k]\n",
    "\n",
    "                cx = (j + 0.5) / f_k\n",
    "                cy = (i + 0.5) / f_k\n",
    "\n",
    "                s_k = self.min_sizes[k]/self.image_size\n",
    "                mean += [cx, cy, s_k, s_k]\n",
    "\n",
    "                s_k_prime = np.sqrt(s_k * (self.max_sizes[k]/self.image_size))\n",
    "                mean += [cx, cy, s_k_prime, s_k_prime]\n",
    "\n",
    "                for ar in self.aspect_ratios[k]:\n",
    "                    mean += [cx, cy, s_k*np.sqrt(ar), s_k/np.sqrt(ar)]\n",
    "                    mean += [cx, cy, s_k/np.sqrt(ar), s_k*np.sqrt(ar)]\n",
    "\n",
    "        output = torch.Tensor(mean).view(-1, 4)\n",
    "        output.clamp_(max=1, min=0)  \n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "012fbf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(loc, dbox_list):\n",
    "    boxes = torch.cat((\n",
    "        dbox_list[:, :2] + loc[:, :2] * 0.1 * dbox_list[:, 2:],\n",
    "        dbox_list[:, 2:] * torch.exp(loc[:, 2:] * 0.2)), dim=1)\n",
    "    \n",
    "    boxes[:, :2] -= boxes[:, 2:] / 2\n",
    "    boxes[:, 2:] += boxes[:, :2]\n",
    "    \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e29e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nm_suppression(boxes, scores, overlap=0.45, top_k=200):\n",
    "    count = 0\n",
    "    keep = scores.new(scores.size(0)).zero_().long()\n",
    "    \n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    area = torch.mul(x2 - x1, y2 - y1)\n",
    "    \n",
    "    tmp_x1 = boxes.new()\n",
    "    tmp_y1 = boxes.new()\n",
    "    tmp_x2 = boxes.new()\n",
    "    tmp_y2 = boxes.new()\n",
    "    tmp_w = boxes.new()\n",
    "    tmp_h = boxes.new()\n",
    "    \n",
    "    v, idx = scores.sort(0)\n",
    "    \n",
    "    idx = idx[-top_k:]\n",
    "    \n",
    "    while idx.numel() > 0:\n",
    "        i = idx[-1]\n",
    "        \n",
    "        keep[count] = i\n",
    "        count += 1\n",
    "        \n",
    "        if idx.size(0) == 1:\n",
    "            break\n",
    "            \n",
    "        idx = idx[:-1]\n",
    "        \n",
    "        torch.index_select(x1, 0, idx, out=tmp_x1)\n",
    "        torch.index_select(y1, 0, idx, out=tmp_y1)\n",
    "        torch.index_select(x2, 0, idx, out=tmp_x2)\n",
    "        torch.index_select(y2, 0, idx, out=tmp_y2)\n",
    "        \n",
    "        tmp_x1 = torch.clamp(tmp_x1, min=x1[i])\n",
    "        tmp_y1 = torch.clamp(tmp_y1, min=y1[i])\n",
    "        tmp_x2 = torch.clamp(tmp_x2, max=x2[i])\n",
    "        tmp_y2 = torch.clamp(tmp_y2, max=y2[i])\n",
    "        \n",
    "        tmp_w.resize_as_(tmp_x2)\n",
    "        tmp_h.resize_as_(tmp_y2)\n",
    "        \n",
    "        tmp_w = tmp_x2 - tmp_x1\n",
    "        tmp_h = tmp_y2 - tmp_y1\n",
    "        \n",
    "        inter = tmp_w*tmp_h\n",
    "        \n",
    "        rem_areas = torch.index_select(area, 0, idx)\n",
    "        union = (rem_areas - inter) + area[i]\n",
    "        IoU = inter/union\n",
    "        \n",
    "        idx = idx[IoU.le(overlap)]\n",
    "        \n",
    "    return keep, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec842bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detect(Function):\n",
    "    def __init__(self, conf_thresh=0.01, top_k=200, nms_thresh=0.45):\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.conf_thresh = conf_thresh\n",
    "        self.top_k = top_k\n",
    "        self.nms_thresh = nms_thresh\n",
    "        \n",
    "    def forward(self, loc_data, conf_data, dbox_list):\n",
    "        num_batch, loc_data.size(0)\n",
    "        num_dbox = loc_data.size(1)\n",
    "        num_classes = conf_data.size(2)\n",
    "        \n",
    "        conf_data = self.softmax(conf_data)\n",
    "        \n",
    "        output = torch.zeros(num_batch, num_classes, self.top_k, 5)\n",
    "        \n",
    "        conf_preds = conf_data.transpose(2, 1)\n",
    "        \n",
    "        for i in range(num_batch):\n",
    "            decoded_boxes = decode(loc_data[i], dbox_list)\n",
    "            \n",
    "            conf_scores = conf_preds[i].close()\n",
    "            \n",
    "            for cl in range(1, num_classes):\n",
    "                c_mask = conf_scores[cl].gt(self.conf_thresh)\n",
    "                \n",
    "                scores = conf_scores[cl][c_mask]\n",
    "                \n",
    "                if scores.nelement() == 0:\n",
    "                    continue\n",
    "                    \n",
    "                l_mask = c_mask.unsqueeze(1).expand_as(decoded_boxes)\n",
    "                \n",
    "                boxes = decoded_boxes[l_mask].view(-1, 4)\n",
    "                \n",
    "                ids, count = nm_suppression(boxes, scores, self.nms_thresh, self.top_k)\n",
    "                \n",
    "                output[i, cl, :count] = torch.cat((scores[ids[:count]].unsqueeze(1), boxes[ids[:count]]), 1)\n",
    "                \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0efdf8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSD(nn.Module):\n",
    "    def __init__(self, phase, cfg):\n",
    "        super(SSD, self).__init__()\n",
    "\n",
    "        self.phase = phase\n",
    "        self.num_classes = cfg['num_classes']\n",
    "\n",
    "        self.vgg = make_vgg()\n",
    "        self.extras = make_extras()\n",
    "        self.L2Norm = L2Norm()\n",
    "        self.loc, self.conf = make_loc_conf(cfg['num_classes'], cfg['bbox_aspect_num'])\n",
    "\n",
    "        dbox = DBox(cfg)\n",
    "        self.dbox_list = dbox.make_dbox_list()\n",
    "\n",
    "        if phase == 'inference':\n",
    "          self.detect = Detect()\n",
    "\n",
    "    def forward(self, x):\n",
    "        sources = list()\n",
    "        loc = list()\n",
    "        conf = list()\n",
    "        \n",
    "        for k in range(23):\n",
    "            x = self.vgg[k](x)\n",
    "            \n",
    "        source1 = self.L2Norm(x)\n",
    "        sources.append(source1)\n",
    "        \n",
    "        for k in range(23, len(self.vgg)):\n",
    "            x = self.vgg[k](x)\n",
    "            \n",
    "        sources.append(x)\n",
    "        \n",
    "        for k, v, in enumerate(self.extras):\n",
    "            x = F.relu(v(x), inplace=True)\n",
    "            if k % 2 == 1:\n",
    "                sources.append(x)\n",
    "                \n",
    "        for (x, l, c) in zip(sources, self.loc, self.conf):\n",
    "            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n",
    "            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n",
    "            \n",
    "        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n",
    "        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n",
    "        \n",
    "        loc = loc.view(loc.size(0), -1, 4)\n",
    "        conf = conf.view(conf.size(0), -1, self.num_classes)\n",
    "        \n",
    "        output = (loc, conf, self.dbox_list)\n",
    "        \n",
    "        if self.phase == 'inference':\n",
    "            return self.detect(output[0], output[1], output[2])\n",
    "        \n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc36554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.match import match\n",
    "\n",
    "class MultiBoxLoss(nn.Module):\n",
    "    def __init__(self, jaccard_thresh=0.5, neg_pos=3, device='cpu'):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "        self.jaccard_thresh = jaccard_thresh\n",
    "        self.negpos_ratio = neg_pos\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, predictions, targets):\n",
    "        loc_data, conf_data, dbox_list = predictions\n",
    "        \n",
    "        num_batch = loc_data.size(0)\n",
    "        num_dbox = loc_data.size(1)\n",
    "        num_classes = conf_data.size(2)\n",
    "        \n",
    "        conf_t_label = torch.LongTensor(num_batch, num_dbox).to(self.device)\n",
    "        loc_t = torch.Tensor(num_batch, num_dbox, 4).to(self.device)\n",
    "        \n",
    "        for idx in range(num_batch):\n",
    "            truths = targets[idx][:, :-1].to(self.device)\n",
    "            labels = targets[idx][:, -1].to(self.device)\n",
    "            \n",
    "            dbox = dbox_list.to(self.device)\n",
    "            \n",
    "            variance = [0.1, 0.2]\n",
    "            match(self.jaccard_thresh, truths, dbox, variance, labels, loc_t, conf_t_label, idx)\n",
    "            \n",
    "        pos_mask = conf_t_label > 0\n",
    "        pos_idx = pos_mask.unsqueeze(pos_mask.dim()).expand_as(loc_data)\n",
    "        \n",
    "        loc_p = loc_data[pos_idx].view(-1, 4)\n",
    "        loc_t = loc_t[pos_idx].view(-1, 4)\n",
    "        \n",
    "        loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction='sum')\n",
    "        \n",
    "        batch_conf = conf_data.view(-1, num_classes)\n",
    "        \n",
    "        loss_c = F.cross_entropy(batch_conf, conf_t_label.view(-1), reduction='none')\n",
    "        \n",
    "        num_pos = pos_mask.long().sum(1, keepdim=True)\n",
    "        \n",
    "        loss_c = loss_c.view(num_batch, -1)\n",
    "        loss_c[pos_mask] = 0\n",
    "        \n",
    "        _, loss_idx = loss_c.sort(1, descending=True)\n",
    "        _, idx_rank = loss_idx.sort(1)\n",
    "        \n",
    "        num_neg = torch.clamp(num_pos*self.negpos_ratio, max=num_dbox)\n",
    "        \n",
    "        neg_mask = idx_rank < (num_neg).expand_as(idx_rank)\n",
    "        \n",
    "        pos_idx_mask = pos_mask.unsqueeze(2).expand_as(conf_data)\n",
    "        neg_idx_mask = neg_mask.unsqueeze(2).expand_as(conf_data)\n",
    "        \n",
    "        conf_hnm = conf_data[(pos_idx_mask+neg_idx_mask).gt(0)].view(-1, num_classes)\n",
    "        \n",
    "        conf_t_label_hnm = conf_t_label[(pos_mask+neg_mask).gt(0)]\n",
    "        \n",
    "        loss_c = F.cross_entropy(conf_hnm, conf_t_label_hnm, reduction='sum')\n",
    "        \n",
    "        N = num_pos.sum()\n",
    "        loss_l /= N\n",
    "        loss_c /= N\n",
    "        \n",
    "        return loss_l, loss_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "080fe1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath = './data/VOCdevkit/VOC2012/'\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(rootpath)\n",
    "\n",
    "voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "                       'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse',\n",
    "                       'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "color_mean = (104, 117, 123)\n",
    "input_size = 300\n",
    "\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list, phase='train', transform=DataTransform(input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase='val', transform=DataTransform(input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn)\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn)\n",
    "\n",
    "dataloaders_dict = {'train': train_dataloader, 'val': val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36f2bae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス: cpu\n"
     ]
    }
   ],
   "source": [
    "ssd_cfg = {\n",
    "    'num_classes': 21,\n",
    "    'input_size': 300,\n",
    "    'bbox_aspect_num': [4, 6, 6, 6, 4, 4],\n",
    "    'feature_maps': [38, 19, 10, 5, 3, 1],\n",
    "    'steps': [8, 16, 32, 64, 100, 300],\n",
    "    'min_sizes': [30, 60, 111, 162, 213, 264],\n",
    "    'max_sizes': [60, 111, 162, 213, 264, 315],\n",
    "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "}\n",
    "\n",
    "net = SSD(phase='train', cfg=ssd_cfg)\n",
    "\n",
    "vgg_weights = torch.load('./weights/vgg16_reducedfc.pth')\n",
    "net.vgg.load_state_dict(vgg_weights)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "net.extras.apply(weights_init)\n",
    "net.loc.apply(weights_init)\n",
    "net.conf.apply(weights_init)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'使用デバイス: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8976b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = MultiBoxLoss(jaccard_thresh=0.5, neg_pos=3, device=device)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7babbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, detaloaders_dict, criterion, optimizer, num_epochs):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'使用デバイス: {device}')\n",
    "    \n",
    "    net.to(device)\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    iteration = 1\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_val_loss = 0.0\n",
    "    logs = []\n",
    "    \n",
    "    for epoch in range(num_epochs+1):\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "        \n",
    "        print('-----------')\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-----------')\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()\n",
    "                print(' (train) ')\n",
    "            else:\n",
    "                if ((epoch+1) % 10 == 0):\n",
    "                    net.eval()\n",
    "                    print('-----------')\n",
    "                    print(' (val) ')\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "            for images, targets in dataloaders_dict[phase]:\n",
    "                images = images.to(device)\n",
    "                targets = [ann.to(device) for ann in targets]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(images)\n",
    "                    \n",
    "                    loss_l, loss_c = criterion(outputs, targets)\n",
    "                    loss = loss_l + loss_c\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        \n",
    "                        nn.utils.clip_grad_value_(net.parameters(), clip_value=2.0)\n",
    "                        \n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        if (iteration % 10 == 0):\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print(f'iteration {iteration} || Loss: {loss.item():.4f} || 10iter: {duration:.4f}')\n",
    "                            t_iter_start = time.time()\n",
    "                            \n",
    "                        epoch_train_loss += loss.item()\n",
    "                        iteration += 1\n",
    "                        \n",
    "                    else:\n",
    "                        epoch_val_loss += loss.item()\n",
    "                        \n",
    "        t_epoch_finish = time.time()\n",
    "        print('-----------')\n",
    "        print(f'epoch {epoch+1} || Epoch_TRAIN_Loss:{epoch_train_loss:.4f} || Epoch_VAL_Loss: {epoch_val_loss:.4f}')\n",
    "        print(f'timer: {t_epoch_finish - t_epoch_start:.4f}')\n",
    "        t_epoch_start = time.time()\n",
    "        \n",
    "        log_epoch = {'epoch': epoch+1, 'train_loss': epoch_train_loss, 'val_loss': epoch_val_loss}\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv('log_output.csv')\n",
    "        \n",
    "        epoch_train_loss = 0.0\n",
    "        epoch_val_loss = 0.0\n",
    "        \n",
    "        if ((epoch+1) % 10 == 0):\n",
    "            torch.save(net.state_dict(), 'weight/ssd300_' + str(epoch+1) + '.pth')\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce939034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス: cpu\n",
      "-----------\n",
      "Epoch 1/50\n",
      "-----------\n",
      " (train) \n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1e918d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
